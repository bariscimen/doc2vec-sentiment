{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Doc2Vec\n",
    "\n",
    "We use Word2Vec for sentiment analysis by attempting to classify the Cornell IMDB movie review corpus (http://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
    "\n",
    "The data used in this demo can be found at https://github.com/bariscimen/doc2vec-sentiment\n",
    "\n",
    "## Setup\n",
    "### Modules\n",
    "We use gensim, since gensim has a much more readable implementation of Word2Vec (and Doc2Vec). Bless those guys. We also use numpy for general array manipulation, and sklearn for Logistic Regression classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format\n",
    "We can't input the raw reviews from the Cornell movie review data repository. Instead, we clean them up by converting everything to lower case and removing punctuation. \n",
    "\n",
    "The result is to have five documents:\n",
    "- test-neg.txt: 12500 negative movie reviews from the test data\n",
    "- test-pos.txt: 12500 positive movie reviews from the test data\n",
    "- train-neg.txt: 12500 negative movie reviews from the training data\n",
    "- train-pos.txt: 12500 positive movie reviews from the training data\n",
    "- train-unsup.txt: 50000 Unlabelled movie reviews\n",
    "\n",
    "** *each document should be on one line, separated by new lines. **\n",
    "\n",
    "### Feeding Data to Doc2Vec\n",
    "Doc2Vec (the portion of gensim that implements the Doc2Vec algorithm) does a great job at word embedding, but a terrible job at reading in files. It only takes in TaggedLineSentence classes which basically yields TaggedDocument, a class from gensim.models.doc2vec representing a single sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TaggedLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield TaggedDocument(words=utils.to_unicode(line).split(), tags=[prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(TaggedDocument(words=utils.to_unicode(line).split(), tags=[prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def shuffle_sentences(self):\n",
    "        return shuffle(self.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can feed the data files to TaggedLineSentence. As we mentioned earlier, TaggedLineSentence simply takes a dictionary with keys as the file names and values the special prefixes for sentences from that document. The prefixes need to be unique, so that there is no ambiguitiy for sentences from different documents.\n",
    "\n",
    "The prefixes will have a counter appended to them to label individual sentences in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = TaggedLineSentence(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Building the Vocabulary Table\n",
    "Doc2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them). So we feed it the array of sentences. model.build_vocab takes an array of TaggedLineSentence, hence our to_array function in the TaggedLineSentence class.\n",
    "If you're curious about the parameters, do read the Word2Vec documentation. Otherwise, here's a quick rundown:\n",
    "- min_count: ignore all words with total frequency lower than this. You have to set this to 1, since the sentence labels only appear once. Setting it any higher than 1 will miss out on the sentences.\n",
    "- window: the maximum distance between the current and predicted word within a sentence. Word2Vec uses a skip-gram model, and this is simply the window size of the skip-gram model.\n",
    "- size: dimensionality of the feature vectors in output. 100 is a good number. If you're extreme, you can go up to around 400.\n",
    "- sample: threshold for configuring which higher-frequency words are randomly downsampled\n",
    "- workers: use this many worker threads to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Doc2Vec\n",
    "Now we train the model. The model is better trained if **in each training epoch, the sequence of sentences fed to the model is randomized.** This is important: missing out on this steps gives you really shitty results. This is the reason for the sentences_perm method in our TaggedLineSentence class.\n",
    "\n",
    "We train it for 10 epochs. If I had more time, I'd have done 20.\n",
    "\n",
    "This process takes around 10 mins, so go grab some coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    sentences.shuffle_sentences()\n",
    "    model.train(sentences.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample vector for the first sentence in the training set for negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32277206, -0.03962494, -0.07266915, -0.3584137 ,  0.20809357,\n",
       "       -0.25993574, -0.11822301,  0.48944992, -0.37939513, -0.22172844,\n",
       "        0.67778319,  0.25943616,  0.18589967,  0.59513271, -0.03071132,\n",
       "        0.16239348,  0.08516458, -0.17490691,  0.44850519, -0.3339214 ,\n",
       "        0.07385798,  0.13084169,  0.10786842, -0.1326807 , -0.13181499,\n",
       "        0.00358524,  0.01887527,  0.0025575 ,  0.3032847 , -0.0722024 ,\n",
       "       -0.27787772, -0.01889877,  0.00405976, -0.19545884, -0.12312309,\n",
       "       -0.276225  , -0.48368922,  0.29648355,  0.04676267,  0.20177564,\n",
       "        0.0226334 , -0.28215566,  0.14990538,  0.1497556 , -0.22867598,\n",
       "       -0.59170425, -0.08138497, -0.51442462, -0.04024356, -0.22482392,\n",
       "        0.16997276, -0.21076104,  0.16284338,  0.26058167,  0.52194506,\n",
       "        0.3672393 , -0.06896825,  0.03472322, -0.05638345, -0.50952393,\n",
       "        0.34179381, -0.05666731,  0.08396357,  0.12766367, -0.16163798,\n",
       "        0.1123985 ,  0.0809842 , -0.3686167 ,  0.13539273,  0.37259695,\n",
       "       -0.43766385,  0.26763034,  0.43779689, -0.05571023,  0.05501395,\n",
       "       -0.1671534 , -0.27345347, -0.28270751, -0.10573871, -0.09029049,\n",
       "       -0.46443379, -0.12799518,  0.07374658,  0.0768398 ,  0.08340514,\n",
       "       -0.15559809, -0.02772764, -0.18890443,  0.24321339,  0.10633512,\n",
       "       -0.30164459, -0.37370342, -0.33833405, -0.34341714,  0.10286835,\n",
       "       -0.38912964,  0.11033975, -0.25611764, -0.409082  ,  0.10312693], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TRAIN_NEG_1'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "To avoid training the model again, we can save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiments\n",
    "### Training Vectors\n",
    "Now let's use these vectors to train a classifier. First, we must extract the training vectors. Remember that we have a total of 25000 training reviews, with equal numbers of positive and negative ones (12500 positive, 12500 negative).\n",
    "\n",
    "Hence, we create a numpy array (since the classifier we use only takes numpy arrays. There are two parallel arrays, one containing the vectors (train_arrays) and the other containing the labels (train_labels).\n",
    "\n",
    "We simply put the positive ones at the first half of the array, and the negative ones at the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((25000, 100))\n",
    "train_labels = numpy.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training array looks like this: rows and rows of vectors representing each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53936642 -0.23031077 -0.03832249 ...,  0.2249258  -0.54613489\n",
      "  -0.11463475]\n",
      " [-0.41936064 -0.16192876  0.19960476 ...,  0.04475639  0.04624066\n",
      "   0.0749617 ]\n",
      " [-0.09952848 -0.2859416   0.12483131 ...,  0.53659874 -0.65958494\n",
      "   0.11224388]\n",
      " ..., \n",
      " [-0.05353339 -0.2487039  -0.02476246 ..., -0.0764806  -0.30736387\n",
      "   0.02775923]\n",
      " [-0.02365136 -0.14059344 -0.08467647 ..., -0.2423065  -0.38618767\n",
      "   0.16995062]\n",
      " [-0.05582325 -0.16865364 -0.26180309 ..., -0.11948675 -0.29945078\n",
      "  -0.19749543]]\n"
     ]
    }
   ],
   "source": [
    "print train_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are simply category labels for the sentence vectors -- 1 representing positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vectors\n",
    "We do the same for testing data -- data that we are going to feed to the classifier after we've trained it using the training data. This allows us to evaluate our results. The process is pretty much the same as extracting the results for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((25000, 100))\n",
    "test_labels = numpy.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Now we train a logistic regression classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And find that we have achieved near 88% accuracy for sentiment analysis. This is rather incredible, given that we are only using a linear SVM and a very shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://github.com/linanqiu/word2vec-sentiments\n",
    "- Doc2vec: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "- Paper that inspired this: http://arxiv.org/abs/1405.4053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
